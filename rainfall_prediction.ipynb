{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Prediction with Rainfall Dataset\n",
    "\n",
    "**Goal**: Build a robust classifier to predict daily rainfall using environmental data.\n",
    "\n",
    "**Approach**: Ensemble of Random Forest, AdaBoost, and Logistic Regression with soft voting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading & Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section handles:\n",
    "- Loading training and test datasets\n",
    "- Basic statistics and data quality checks\n",
    "- Visualization of feature distributions\n",
    "- Correlation analysis\n",
    "- Class imbalance examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"\\nTraining data has {train_df.shape[0]} samples with {train_df.shape[1]} features\")\n",
    "print(f\"Test data has {test_df.shape[0]} samples with {test_df.shape[1] - 1} features (no target)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 5 rows of training data:\")\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\nFirst 5 rows of test data:\")\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data info:\")\n",
    "train_df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nTest data info:\")\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in training data:\")\n",
    "missing_train = train_df.isnull().sum()\n",
    "print(missing_train)\n",
    "print(f\"\\nTotal missing values: {missing_train.sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nMissing values in test data:\")\n",
    "missing_test = test_df.isnull().sum()\n",
    "print(missing_test)\n",
    "print(f\"\\nTotal missing values: {missing_test.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical Summary of Training Data:\")\n",
    "display(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Class Distribution & Imbalance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check target variable distribution\n",
    "rainfall_counts = train_df['rainfall'].value_counts()\n",
    "rainfall_pcts = train_df['rainfall'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Rainfall Class Distribution:\")\n",
    "print(rainfall_counts)\n",
    "print(f\"\\nClass Percentages:\")\n",
    "print(f\"Rain (1): {rainfall_pcts[1]:.2f}%\")\n",
    "print(f\"No Rain (0): {rainfall_pcts[0]:.2f}%\")\n",
    "print(f\"\\nImbalance Ratio: {rainfall_counts[1] / rainfall_counts[0]:.2f}:1\")\n",
    "print(\"\\n Significant class imbalance found. Will need to address with class_weight='balanced.'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# count plot\n",
    "sns.countplot(data=train_df, x='rainfall', ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Class Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Rainfall (0=No, 1=Yes)', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xticklabels(['No Rain', 'Rain'])\n",
    "\n",
    "# add count labels on bars\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container)\n",
    "\n",
    "# pie chart\n",
    "axes[1].pie(rainfall_counts, labels=['No Rain', 'Rain'], autopct='%1.1f%%', \n",
    "            startangle=90, colors=sns.color_palette('Set2'))\n",
    "axes[1].set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude id, day, rainfall\n",
    "feature_cols = ['pressure', 'maxtemp', 'temparature', 'mintemp', 'dewpoint', \n",
    "                'humidity', 'cloud', 'sunshine', 'winddirection', 'windspeed']\n",
    "\n",
    "# plot distributions\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 18))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(feature_cols):\n",
    "    axes[idx].hist(train_df[col], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col, fontsize=10)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Feature Distributions by Rainfall Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plots comparing feature distributions between rain, no-rain days\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 18))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(feature_cols):\n",
    "    sns.boxplot(data=train_df, x='rainfall', y=col, ax=axes[idx], palette='Set2')\n",
    "    axes[idx].set_title(f'{col} by Rainfall', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Rainfall (0=No, 1=Yes)', fontsize=10)\n",
    "    axes[idx].set_xticklabels(['No Rain', 'Rain'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = train_df[feature_cols + ['rainfall']].corr()\n",
    "\n",
    "# visualize correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features most correlated with rainfall\n",
    "rainfall_corr = correlation_matrix['rainfall'].drop('rainfall').sort_values(ascending=False)\n",
    "\n",
    "print(\"Features correlation with Rainfall (sorted):\")\n",
    "print(rainfall_corr)\n",
    "\n",
    "# visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "rainfall_corr.plot(kind='barh', color='teal', edgecolor='black')\n",
    "plt.title('Feature Correlation with Rainfall', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Key Insights from EDA\n",
    "\n",
    "**Summary of findings**:\n",
    "- Dataset has 2190 training samples and 730 test samples\n",
    "- Significant class imbalance: 75.3% rain vs 24.7% no rain\n",
    "- Features show varying distributions and correlations with rainfall\n",
    "- Strong multicollinearity expected between temperature variables (maxtemp, temparature, mintemp, dewpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Data Preprocessing & Feature Engineering\n",
    "\n",
    "This section handles:\n",
    "- Dropping non-predictive features (id, day)\n",
    "- Feature engineering (creating new features)\n",
    "- Feature scaling (standardization)\n",
    "- Train/validation split with stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Create new features from existing ones.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with raw features\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with engineered features added\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # temperature range (diurnal temperature variation)\n",
    "    df['temp_range'] = df['maxtemp'] - df['mintemp']\n",
    "    \n",
    "    # dewpoint depression (how close air is to saturation)\n",
    "    # lower values = air closer to saturation = more likely to rain\n",
    "    df['dewpoint_depression'] = df['temparature'] - df['dewpoint']\n",
    "    \n",
    "    # temperature deviation from daily average\n",
    "    df['temp_from_avg'] = df['temparature'] - (df['maxtemp'] + df['mintemp']) / 2\n",
    "    \n",
    "    # interaction: high humidity with low dewpoint depression\n",
    "    df['humidity_dewpoint_interaction'] = df['humidity'] * (1 / (df['dewpoint_depression'] + 1))\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Applying feature engineering...\")\n",
    "train_engineered = engineer_features(train_df)\n",
    "test_engineered = engineer_features(test_df)\n",
    "\n",
    "print(f\"\\nOriginal training features: {train_df.shape[1]}\")\n",
    "print(f\"After feature engineering: {train_engineered.shape[1]}\")\n",
    "print(f\"\\nNew features added: {list(set(train_engineered.columns) - set(train_df.columns))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine new features\n",
    "print(\"Sample of engineered features:\")\n",
    "display(train_engineered[['temp_range', 'dewpoint_depression', 'temp_from_avg', \n",
    "                          'humidity_dewpoint_interaction', 'rainfall']].head(10))\n",
    "\n",
    "# check correlation of new features with rainfall\n",
    "new_features = ['temp_range', 'dewpoint_depression', 'temp_from_avg', 'humidity_dewpoint_interaction']\n",
    "new_feature_corr = train_engineered[new_features + ['rainfall']].corr()['rainfall'].drop('rainfall').sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nNew features correlation with Rainfall:\")\n",
    "print(new_feature_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing winddirection value in test data\n",
    "# use median from training data to avoid data leakage\n",
    "\n",
    "winddirection_median = train_engineered['winddirection'].median()\n",
    "\n",
    "print(f\"Missing values before imputation:\")\n",
    "print(f\"Training set: {train_engineered['winddirection'].isnull().sum()}\")\n",
    "print(f\"Test set: {test_engineered['winddirection'].isnull().sum()}\")\n",
    "\n",
    "# apply median imputation to test set\n",
    "test_engineered['winddirection'].fillna(winddirection_median, inplace=True)\n",
    "\n",
    "print(f\"\\nImputation value used: {winddirection_median}\")\n",
    "print(f\"\\nMissing values after imputation:\")\n",
    "print(f\"Training set: {train_engineered['winddirection'].isnull().sum()}\")\n",
    "print(f\"Test set: {test_engineered['winddirection'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-predictive columns\n",
    "columns_to_drop = ['id', 'day']\n",
    "\n",
    "# separate features and target for training data\n",
    "X = train_engineered.drop(columns=columns_to_drop + ['rainfall'])\n",
    "y = train_engineered['rainfall']\n",
    "\n",
    "# prepare test data (no target)\n",
    "X_test = test_engineered.drop(columns=columns_to_drop)\n",
    "test_ids = test_engineered['id']\n",
    "\n",
    "print(f\"Training features shape: {X.shape}\")\n",
    "print(f\"Training target shape: {y.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "print(f\"\\nFeatures used for modeling:\")\n",
    "print(list(X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train/Validation Split (Stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split with stratification to maintain class distribution\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set size: {X_val.shape[0]} samples\")\n",
    "\n",
    "# verify stratification maintained class distribution\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "\n",
    "print(f\"\\nValidation set class distribution:\")\n",
    "print(y_val.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Feature Scaling (Standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit on training data only, then transform all sets\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# convert back to df\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X.columns, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(\"Feature scaling completed.\")\n",
    "print(f\"\\nScaled training features (first 5 rows):\")\n",
    "display(X_train_scaled.head())\n",
    "\n",
    "# verify scaling: mean ~ 0, std ~ 1\n",
    "print(\"\\nVerifying standardization (should be ~0 mean, ~1 std):\")\n",
    "print(f\"Mean of scaled features:\\n{X_train_scaled.mean().round(6)}\")\n",
    "print(f\"\\nStd of scaled features:\\n{X_train_scaled.std().round(6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Final Preprocessed Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n Dataset Splits:\")\n",
    "print(f\"  - Training set: {X_train_scaled.shape[0]} samples\")\n",
    "print(f\"  - Validation set: {X_val_scaled.shape[0]} samples\")\n",
    "print(f\"  - Test set: {X_test_scaled.shape[0]} samples\")\n",
    "\n",
    "print(f\"\\n Feature Engineering:\")\n",
    "print(f\"  - Original features: 10\")\n",
    "print(f\"  - Engineered features: 4\")\n",
    "print(f\"  - Total features: {X_train_scaled.shape[1]}\")\n",
    "\n",
    "print(f\"\\n Applied Transformations:\")\n",
    "print(f\"  - Dropped non-predictive columns: id, day\")\n",
    "print(f\"  - Created engineered features: temp_range, dewpoint_depression, temp_from_avg, humidity_dewpoint_interaction\")\n",
    "print(f\"  - Standardized all features (mean=0, std=1)\")\n",
    "print(f\"  - Stratified train/validation split (80/20)\")\n",
    "\n",
    "print(f\"\\n Class Imbalance:\")\n",
    "print(f\"  - Rain: 75.3%, No Rain: 24.7%\")\n",
    "print(f\"  - Will use class_weight='balanced' in models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Classifiers-\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight='balanced')\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "\n",
    "\n",
    "# Train the models\n",
    "\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "ada.fit(X_train_scaled, y_train)\n",
    "logreg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Ensemble Classifier Through Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Create a Voting Classifier combining the three models\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf),\n",
    "        ('ada', ada),\n",
    "        ('logreg', logreg)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": rf,\n",
    "    \"AdaBoost\": ada,\n",
    "    \"Logistic Regression\": logreg,\n",
    "    \"Voting Ensemble\": voting_clf\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"{name} Accuracy: {scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Ensemble Model Training and Performance Evaluation Based on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "voting_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict training probabilities\n",
    "y_proba = voting_clf.predict_proba(X_train_scaled)[:, 1]\n",
    "\n",
    "# Compute ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(\"Training AUC:\", roc_auc)\n",
    "\n",
    "# Random Guess line:\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=1, linestyle='--')\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Training Data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Generate predictions on test set and evaluate performance\n",
    "\n",
    "This section handles:\n",
    "- Running prediction on trained algorithm\n",
    "- Compute the accuracy score on algorithm\n",
    "- Construct Confusion matrix and analysis them\n",
    "- Construct ROC and Calculate AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Running Prediction on trained algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf_prediction = voting_clf.predict(X_val_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Compute the accuracy score on algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_val, voting_clf_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Construct Confusion matrix and analysis them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cmd = ConfusionMatrixDisplay.from_predictions(y_val,voting_clf_prediction)\n",
    "cmd.ax_.grid(False)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Of the 77 + 31 = 108 observations that did not have rainfall, 77/108 (**71.3%**) were correctly classified, 31/108 (**28.7%**) were not correctly classified, therefore our **True Negative Rate(TNR)** is 77/108(**71.3%**). \n",
    "\n",
    "#### Of the 30 + 300 = 330 observation that does have rainfall, 300/330 (**90.6%**) were correctly classified, and 30/330 (**8.4%**) were not correctly classified, therefore our **True Positive Rate(TPR)** is 90.6%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_val, voting_clf_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve and AUC for the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict validation set probabilities\n",
    "y_val_proba = voting_clf.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_val_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"Ensemble Validation AUC: {roc_auc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Random guess line\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=1, linestyle='--')\n",
    "# Plot our model's ROC curve\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Axis\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "\n",
    "# Title\n",
    "plt.title(\"Ensemble ROC Curve (Validation Data)\", fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Individual model roc curve comparison utilizing validation set\n",
    "y_val_proba_rf = rf.predict_proba(X_val_scaled)[:, 1]\n",
    "fpr_rf, tpr_rf, thresholds = roc_curve(y_val, y_val_proba_rf)\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "print(f\"Random Forest Validation AUC: {roc_auc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Random guess line\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=1, linestyle='--')\n",
    "# Plot our model's ROC curve\n",
    "plt.plot(fpr_rf, tpr_rf, color='red', lw=2, label=f'ROC curve (area = {roc_auc_rf:.2f})')\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Axis\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "\n",
    "# Title\n",
    "plt.title(\"Random Forest ROC Curve (Validation Data)\", fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "y_val_proba_ada = ada.predict_proba(X_val_scaled)[:, 1]\n",
    "fpr_ada, tpr_ada, thresholds = roc_curve(y_val, y_val_proba_ada)\n",
    "roc_auc_ada = auc(fpr_ada, tpr_ada)\n",
    "\n",
    "print(f\"AdaBoost Validation AUC: {roc_auc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Random guess line\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=1, linestyle='--')\n",
    "# Plot our model's ROC curve\n",
    "plt.plot(fpr_ada, tpr_ada, color='red', lw=2, label=f'ROC curve (area = {roc_auc_ada:.2f})')\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Axis\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "\n",
    "# Title\n",
    "plt.title(\"AdaBoost ROC Curve (Validation Data)\", fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "y_val_proba_logreg = logreg.predict_proba(X_val_scaled)[:, 1]\n",
    "fpr_lr, tpr_lr, thresholds = roc_curve(y_val, y_val_proba_logreg)\n",
    "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "print(f\"Logistic Regression Validation AUC: {roc_auc_lr:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Random guess line\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=1, linestyle='--')\n",
    "# Plot our model's ROC curve\n",
    "plt.plot(fpr_lr, tpr_lr, color='red', lw=2, label=f'ROC curve (area = {roc_auc_lr:.2f})')\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Axis\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "\n",
    "# Title\n",
    "plt.title(\"Logistic Regression ROC Curve (Validation Data)\", fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
