{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Prediction with Rainfall Dataset\n",
    "\n",
    "**Goal**: Build a robust classifier to predict daily rainfall using environmental data.\n",
    "\n",
    "**Approach**: Ensemble of Random Forest, AdaBoost, and Logistic Regression with soft voting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading & Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section handles:\n",
    "- Loading training and test datasets\n",
    "- Basic statistics and data quality checks\n",
    "- Visualization of feature distributions\n",
    "- Correlation analysis\n",
    "- Class imbalance examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('https://storage.googleapis.com/kagglesdsdata/competitions/91714/11251744/train.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1764276230&Signature=VPDP9kOA7bV9QqXOUp2HE85r5JoTvn6ZKQoLBYSpok1PGWrtCx15nSB7s1eniVhg0vypnZEIy1FT1AHelVsMFjB9wvPyke8o05heYkXkWcQzt%2BG9BJeergycy3X%2B2ENqy5x2lKhlv1r2TiLVjt7LPqRd%2FYUzCewmrCNxYs%2BsyXIdacWZRfyvMcpW4tgXaOin6QBwDpbZdQLEsoQ%2BDleO%2BvZsl1kEYUA159pdzCDY%2BEOA81f20j7jZsGIdJR3k%2FB5mTvqF3EF8%2BrIf4px5eCzjKg4AOmXEeDXpFYCC4s0dGWIpWVlohG95wRfqSw7c9d4ElWGjT1IYI%2BUSG0syaNGLA%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain.csv')\n",
    "test_df = pd.read_csv('https://storage.googleapis.com/kagglesdsdata/competitions/91714/11251744/test.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1764277212&Signature=iV0LMt8woV4SFYatlFCdIQfBKy6fgQ94AFdZVfowDy6uCcEp9fhnkd4PbxB7pRk0yzACnPAudyck5ErHoUS%2FepnqS8LKliBkij%2FpP7zJ2%2FDUf0b6hWNcV20LvCyEQzB3ErZkc8ECMyEcgrFo9w4H8Q%2Bdklx0R5w9bqCNw70UXvtOIPLC8jiLj%2Fz7U%2B5JnfwKH8SLVGutNLKcUiMnPt0QfZBW7TLrYAIklnHZB3JJqePNQNwoAV0Iywg6lopvO8YPHuoaMq1xO5Oi3OKSLGks9QW109vITvrZKSdYA7noomvty27uLEgiFuGCyOK9LqysySTYn7EOF%2FDp3uMJoWidKQ%3D%3D&response-content-disposition=attachment%3B+filename%3Dtest.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"\\nTraining data has {train_df.shape[0]} samples with {train_df.shape[1]} features\")\n",
    "print(f\"Test data has {test_df.shape[0]} samples with {test_df.shape[1] - 1} features (no target)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 5 rows of training data:\")\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\nFirst 5 rows of test data:\")\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data info:\")\n",
    "train_df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nTest data info:\")\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in training data:\")\n",
    "missing_train = train_df.isnull().sum()\n",
    "print(missing_train)\n",
    "print(f\"\\nTotal missing values: {missing_train.sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nMissing values in test data:\")\n",
    "missing_test = test_df.isnull().sum()\n",
    "print(missing_test)\n",
    "print(f\"\\nTotal missing values: {missing_test.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical Summary of Training Data:\")\n",
    "display(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Class Distribution & Imbalance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check target variable distribution\n",
    "rainfall_counts = train_df['rainfall'].value_counts()\n",
    "rainfall_pcts = train_df['rainfall'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Rainfall Class Distribution:\")\n",
    "print(rainfall_counts)\n",
    "print(f\"\\nClass Percentages:\")\n",
    "print(f\"Rain (1): {rainfall_pcts[1]:.2f}%\")\n",
    "print(f\"No Rain (0): {rainfall_pcts[0]:.2f}%\")\n",
    "print(f\"\\nImbalance Ratio: {rainfall_counts[1] / rainfall_counts[0]:.2f}:1\")\n",
    "print(\"\\n Significant class imbalance found. Will need to address with class_weight='balanced.'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# count plot\n",
    "sns.countplot(data=train_df, x='rainfall', ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Class Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Rainfall (0=No, 1=Yes)', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xticklabels(['No Rain', 'Rain'])\n",
    "\n",
    "# add count labels on bars\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container)\n",
    "\n",
    "# pie chart\n",
    "axes[1].pie(rainfall_counts, labels=['No Rain', 'Rain'], autopct='%1.1f%%', \n",
    "            startangle=90, colors=sns.color_palette('Set2'))\n",
    "axes[1].set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude id, day, rainfall\n",
    "feature_cols = ['pressure', 'maxtemp', 'temparature', 'mintemp', 'dewpoint', \n",
    "                'humidity', 'cloud', 'sunshine', 'winddirection', 'windspeed']\n",
    "\n",
    "# plot distributions\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 18))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(feature_cols):\n",
    "    axes[idx].hist(train_df[col], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col, fontsize=10)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Feature Distributions by Rainfall Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plots comparing feature distributions between rain, no-rain days\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 18))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(feature_cols):\n",
    "    sns.boxplot(data=train_df, x='rainfall', y=col, ax=axes[idx], palette='Set2')\n",
    "    axes[idx].set_title(f'{col} by Rainfall', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Rainfall (0=No, 1=Yes)', fontsize=10)\n",
    "    axes[idx].set_xticklabels(['No Rain', 'Rain'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = train_df[feature_cols + ['rainfall']].corr()\n",
    "\n",
    "# visualize correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features most correlated with rainfall\n",
    "rainfall_corr = correlation_matrix['rainfall'].drop('rainfall').sort_values(ascending=False)\n",
    "\n",
    "print(\"Features correlation with Rainfall (sorted):\")\n",
    "print(rainfall_corr)\n",
    "\n",
    "# visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "rainfall_corr.plot(kind='barh', color='teal', edgecolor='black')\n",
    "plt.title('Feature Correlation with Rainfall', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Key Insights from EDA\n",
    "\n",
    "**Summary of findings**:\n",
    "- Dataset has 2190 training samples and 730 test samples\n",
    "- Significant class imbalance: 75.3% rain vs 24.7% no rain\n",
    "- Features show varying distributions and correlations with rainfall\n",
    "- Strong multicollinearity expected between temperature variables (maxtemp, temparature, mintemp, dewpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Data Preprocessing & Feature Engineering\n",
    "\n",
    "This section handles:\n",
    "- Dropping non-predictive features (id, day)\n",
    "- Feature engineering (creating new features)\n",
    "- Feature scaling (standardization)\n",
    "- Train/validation split with stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Create new features from existing ones.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with raw features\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with engineered features added\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # temperature range (diurnal temperature variation)\n",
    "    df['temp_range'] = df['maxtemp'] - df['mintemp']\n",
    "    \n",
    "    # dewpoint depression (how close air is to saturation)\n",
    "    # lower values = air closer to saturation = more likely to rain\n",
    "    df['dewpoint_depression'] = df['temparature'] - df['dewpoint']\n",
    "    \n",
    "    # temperature deviation from daily average\n",
    "    df['temp_from_avg'] = df['temparature'] - (df['maxtemp'] + df['mintemp']) / 2\n",
    "    \n",
    "    # interaction: high humidity with low dewpoint depression\n",
    "    df['humidity_dewpoint_interaction'] = df['humidity'] * (1 / (df['dewpoint_depression'] + 1))\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Applying feature engineering...\")\n",
    "train_engineered = engineer_features(train_df)\n",
    "test_engineered = engineer_features(test_df)\n",
    "\n",
    "print(f\"\\nOriginal training features: {train_df.shape[1]}\")\n",
    "print(f\"After feature engineering: {train_engineered.shape[1]}\")\n",
    "print(f\"\\nNew features added: {list(set(train_engineered.columns) - set(train_df.columns))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine new features\n",
    "print(\"Sample of engineered features:\")\n",
    "display(train_engineered[['temp_range', 'dewpoint_depression', 'temp_from_avg', \n",
    "                          'humidity_dewpoint_interaction', 'rainfall']].head(10))\n",
    "\n",
    "# check correlation of new features with rainfall\n",
    "new_features = ['temp_range', 'dewpoint_depression', 'temp_from_avg', 'humidity_dewpoint_interaction']\n",
    "new_feature_corr = train_engineered[new_features + ['rainfall']].corr()['rainfall'].drop('rainfall').sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nNew features correlation with Rainfall:\")\n",
    "print(new_feature_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing winddirection value in test data\n",
    "# use median from training data to avoid data leakage\n",
    "\n",
    "winddirection_median = train_engineered['winddirection'].median()\n",
    "\n",
    "print(f\"Missing values before imputation:\")\n",
    "print(f\"Training set: {train_engineered['winddirection'].isnull().sum()}\")\n",
    "print(f\"Test set: {test_engineered['winddirection'].isnull().sum()}\")\n",
    "\n",
    "# apply median imputation to test set\n",
    "test_engineered['winddirection'].fillna(winddirection_median, inplace=True)\n",
    "\n",
    "print(f\"\\nImputation value used: {winddirection_median}\")\n",
    "print(f\"\\nMissing values after imputation:\")\n",
    "print(f\"Training set: {train_engineered['winddirection'].isnull().sum()}\")\n",
    "print(f\"Test set: {test_engineered['winddirection'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-predictive columns\n",
    "columns_to_drop = ['id', 'day']\n",
    "\n",
    "# separate features and target for training data\n",
    "X = train_engineered.drop(columns=columns_to_drop + ['rainfall'])\n",
    "y = train_engineered['rainfall']\n",
    "\n",
    "# prepare test data (no target)\n",
    "X_test = test_engineered.drop(columns=columns_to_drop)\n",
    "test_ids = test_engineered['id']\n",
    "\n",
    "print(f\"Training features shape: {X.shape}\")\n",
    "print(f\"Training target shape: {y.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "print(f\"\\nFeatures used for modeling:\")\n",
    "print(list(X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train/Validation Split (Stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split with stratification to maintain class distribution\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set size: {X_val.shape[0]} samples\")\n",
    "\n",
    "# verify stratification maintained class distribution\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "\n",
    "print(f\"\\nValidation set class distribution:\")\n",
    "print(y_val.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Feature Scaling (Standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit on training data only, then transform all sets\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# convert back to df\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X.columns, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(\"Feature scaling completed.\")\n",
    "print(f\"\\nScaled training features (first 5 rows):\")\n",
    "display(X_train_scaled.head())\n",
    "\n",
    "# verify scaling: mean ~ 0, std ~ 1\n",
    "print(\"\\nVerifying standardization (should be ~0 mean, ~1 std):\")\n",
    "print(f\"Mean of scaled features:\\n{X_train_scaled.mean().round(6)}\")\n",
    "print(f\"\\nStd of scaled features:\\n{X_train_scaled.std().round(6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Final Preprocessed Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n Dataset Splits:\")\n",
    "print(f\"  - Training set: {X_train_scaled.shape[0]} samples\")\n",
    "print(f\"  - Validation set: {X_val_scaled.shape[0]} samples\")\n",
    "print(f\"  - Test set: {X_test_scaled.shape[0]} samples\")\n",
    "\n",
    "print(f\"\\n Feature Engineering:\")\n",
    "print(f\"  - Original features: 10\")\n",
    "print(f\"  - Engineered features: 4\")\n",
    "print(f\"  - Total features: {X_train_scaled.shape[1]}\")\n",
    "\n",
    "print(f\"\\n Applied Transformations:\")\n",
    "print(f\"  - Dropped non-predictive columns: id, day\")\n",
    "print(f\"  - Created engineered features: temp_range, dewpoint_depression, temp_from_avg, humidity_dewpoint_interaction\")\n",
    "print(f\"  - Standardized all features (mean=0, std=1)\")\n",
    "print(f\"  - Stratified train/validation split (80/20)\")\n",
    "\n",
    "print(f\"\\n Class Imbalance:\")\n",
    "print(f\"  - Rain: 75.3%, No Rain: 24.7%\")\n",
    "print(f\"  - Will use class_weight='balanced' in models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Classifiers-\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight='balanced')\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "\n",
    "\n",
    "# Train the models\n",
    "\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "ada.fit(X_train_scaled, y_train)\n",
    "logreg.fit(X_train_scaled, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "\n",
    "\n",
    "**Part 4**: Build ensemble classifier with soft voting\n",
    "\n",
    "**Part 5**: Generate predictions on test set and evaluate performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
